# -------------------- global -------------------- #

path_save : "/cosmos_storage/home/dlopez/Projects/CL_inference/models/join_inference_CL_Wein_models_illustris_eagle_kmax_0.6"
    
# -------------------- dataset -------------------- #
    
path_load               : "/cosmos_storage/home/dlopez/Projects/CL_inference/DATASETS/DATASET_kmax_0.6/"
list_model_names        : ["Model_fixed_illustris", "Model_fixed_eagle"]
normalize               : True
NN_augs_batch           : 2
add_noise_Pk            : "cosmic_var_gauss"
kmax                    : 0.6
include_baryon_params   : False

# -------------------- arquitechture -------------------- #
    
train_mode                : "train_CL_and_inference"
inference_loss            : "MultivariateNormal" # this will affect the network arquitechture
load_encoder_model_path   : "/cosmos_storage/home/dlopez/Projects/CL_inference/models/only_CL_Wein_models_illustris_eagle_kmax_0.6/neat-sweep-30/model_encoder.pt"
input_encoder             : 99
hidden_layers_encoder     : [100, 100, 64]
output_encoder            : 8
hidden_layers_projector   : [64, 100]
output_projector          : 128
hidden_layers_inference   : [64, 64, 64]
NN_params_out             : 5
load_projector_model_path : None
load_inference_model_path : "/cosmos_storage/home/dlopez/Projects/CL_inference/models/only_inference_CL_Wein_models_illustris_eagle_kmax_0.6/earnest-sweep-5/model_inference.pt"

# -------------------- training -------------------- #

NN_epochs            : 300
NN_batches_per_epoch : 512
batch_size           : 256
weight_decay         : 0.
seed_mode            : "random"
seed                 : 0
lr                   : 0.00119
clip_grad_norm       : 100.

# -------------------- loss -------------------- # 

CL_loss : "Weinberger"
loss_hyperparameters : {
    'c_CL'       : 100.,
    'c_inference': 100.,
    'delta_pull' : 0.5,
    'delta_push' : 1.5,
    'c_pull'     : 1.,
    'c_push'     : 1.,
    'c_reg'      : 0.001,
    '_epsilon'   : 0.00000001
}